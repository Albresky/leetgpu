//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-36424714
// Cuda compilation tools, release 13.0, V13.0.88
// Based on NVVM 7.0.1
//

.version 9.0
.target sm_89
.address_size 64

	// .globl	_Z10vector_addPKfS0_Pfi

.visible .entry _Z10vector_addPKfS0_Pfi(
	.param .u64 _Z10vector_addPKfS0_Pfi_param_0,
	.param .u64 _Z10vector_addPKfS0_Pfi_param_1,
	.param .u64 _Z10vector_addPKfS0_Pfi_param_2,
	.param .u32 _Z10vector_addPKfS0_Pfi_param_3
)
{
	.reg .pred 	%p<7>;
	.reg .f32 	%f<36>;
	.reg .b32 	%r<33>;
	.reg .b64 	%rd<38>;


	ld.param.u64 	%rd22, [_Z10vector_addPKfS0_Pfi_param_0];
	ld.param.u64 	%rd23, [_Z10vector_addPKfS0_Pfi_param_1];
	ld.param.u64 	%rd24, [_Z10vector_addPKfS0_Pfi_param_2];
	ld.param.u32 	%r15, [_Z10vector_addPKfS0_Pfi_param_3];
	cvta.to.global.u64 	%rd1, %rd24;
	cvta.to.global.u64 	%rd2, %rd23;
	cvta.to.global.u64 	%rd3, %rd22;
	mov.u32 	%r16, %ntid.x;
	mov.u32 	%r17, %ctaid.x;
	mul.lo.s32 	%r1, %r17, %r16;
	mov.u32 	%r2, %tid.x;
	add.s32 	%r3, %r1, %r2;
	shr.s32 	%r18, %r15, 31;
	shr.u32 	%r19, %r18, 30;
	add.s32 	%r20, %r15, %r19;
	shr.s32 	%r4, %r20, 2;
	setp.ge.s32 	%p1, %r3, %r4;
	@%p1 bra 	$L__BB0_2;

	mul.wide.s32 	%rd25, %r3, 16;
	add.s64 	%rd26, %rd3, %rd25;
	ld.global.v4.f32 	{%f1, %f2, %f3, %f4}, [%rd26];
	add.s64 	%rd27, %rd2, %rd25;
	ld.global.v4.f32 	{%f9, %f10, %f11, %f12}, [%rd27];
	add.s64 	%rd28, %rd1, %rd25;
	add.f32 	%f17, %f4, %f12;
	add.f32 	%f18, %f3, %f11;
	add.f32 	%f19, %f2, %f10;
	add.f32 	%f20, %f1, %f9;
	st.global.v4.f32 	[%rd28], {%f20, %f19, %f18, %f17};

$L__BB0_2:
	shl.b32 	%r5, %r4, 2;
	add.s32 	%r31, %r5, %r3;
	setp.ge.s32 	%p2, %r31, %r15;
	@%p2 bra 	$L__BB0_9;

	sub.s32 	%r21, %r15, %r1;
	sub.s32 	%r22, %r21, %r2;
	and.b32  	%r30, %r22, 3;
	setp.eq.s32 	%p3, %r30, 0;
	@%p3 bra 	$L__BB0_6;

	add.s32 	%r23, %r2, %r5;
	add.s32 	%r24, %r23, %r1;
	mul.wide.s32 	%rd29, %r24, 4;
	add.s64 	%rd34, %rd1, %rd29;
	add.s64 	%rd33, %rd2, %rd29;
	add.s64 	%rd32, %rd3, %rd29;

$L__BB0_5:
	.pragma "nounroll";
	ld.global.f32 	%f21, [%rd33];
	ld.global.f32 	%f22, [%rd32];
	add.f32 	%f23, %f22, %f21;
	st.global.f32 	[%rd34], %f23;
	add.s32 	%r31, %r31, 1;
	add.s64 	%rd34, %rd34, 4;
	add.s64 	%rd33, %rd33, 4;
	add.s64 	%rd32, %rd32, 4;
	add.s32 	%r30, %r30, -1;
	setp.ne.s32 	%p4, %r30, 0;
	@%p4 bra 	$L__BB0_5;

$L__BB0_6:
	not.b32 	%r25, %r1;
	add.s32 	%r26, %r25, %r15;
	sub.s32 	%r27, %r26, %r2;
	sub.s32 	%r28, %r27, %r5;
	setp.lt.u32 	%p5, %r28, 3;
	@%p5 bra 	$L__BB0_9;

	mul.wide.s32 	%rd30, %r31, 4;
	add.s64 	%rd31, %rd30, 8;
	add.s64 	%rd37, %rd3, %rd31;
	add.s64 	%rd36, %rd2, %rd31;
	add.s64 	%rd35, %rd1, %rd31;

$L__BB0_8:
	ld.global.f32 	%f24, [%rd36+-8];
	ld.global.f32 	%f25, [%rd37+-8];
	add.f32 	%f26, %f25, %f24;
	st.global.f32 	[%rd35+-8], %f26;
	ld.global.f32 	%f27, [%rd36+-4];
	ld.global.f32 	%f28, [%rd37+-4];
	add.f32 	%f29, %f28, %f27;
	st.global.f32 	[%rd35+-4], %f29;
	ld.global.f32 	%f30, [%rd36];
	ld.global.f32 	%f31, [%rd37];
	add.f32 	%f32, %f31, %f30;
	st.global.f32 	[%rd35], %f32;
	ld.global.f32 	%f33, [%rd36+4];
	ld.global.f32 	%f34, [%rd37+4];
	add.f32 	%f35, %f34, %f33;
	st.global.f32 	[%rd35+4], %f35;
	add.s64 	%rd37, %rd37, 16;
	add.s64 	%rd36, %rd36, 16;
	add.s64 	%rd35, %rd35, 16;
	add.s32 	%r31, %r31, 4;
	setp.lt.s32 	%p6, %r31, %r15;
	@%p6 bra 	$L__BB0_8;

$L__BB0_9:
	ret;

}

